{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aclImdb/test/pos/*.txt\n",
      "aclImdb/test/neg/*.txt\n",
      "aclImdb/train/pos/*.txt\n",
      "aclImdb/train/neg/*.txt\n",
      "Done.\n",
      "I cannot understand the need to jump backwards and forwards to scene set, and pad out the plot. Showing that someone has a skill right before they use it, I believe, is offending our intelligence. It's starting to feel a little contrived, and as though they are making up for being so vague for the first three series. A little disappointing this episode.<br /><br />Furthermore, using past quirks, like Locke's ability to know when a storm is ending, is frankly insulting... are we supposed to ooh and arr, or laugh at the softer side of Locke?<br /><br />This episode was all over the place.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "\n",
    "def load_file(fil):\n",
    "    with open(fil, \"r\") as filw:\n",
    "        txt = \" \".join(filw.readlines())\n",
    "        return txt\n",
    "\n",
    "dirn = \"aclImdb\"\n",
    "\n",
    "subdirs_1 = [\"test\", \"train\"]\n",
    "\n",
    "subdirs_2 = [\"pos\", \"neg\"]\n",
    "\n",
    "filenames = {}\n",
    "\n",
    "for route in subdirs_1:\n",
    "    p = os.path.join(dirn, route)\n",
    "    filenames[route] = {}\n",
    "    for sent in subdirs_2:\n",
    "        filenames[route][sent] = glob.glob(os.path.join(p, sent, \"*.txt\"))\n",
    "        print(os.path.join(p, sent, \"*.txt\"))\n",
    "        filenames[route][sent] = [load_file(fil) for fil in filenames[route][sent]]\n",
    "            \n",
    "print(\"Done.\")\n",
    "print(filenames[\"test\"][\"pos\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "    \n",
    "\n",
    "\n",
    "# from tf.keras.models import Sequential  # This does not work!\n",
    "import tensorflow.contrib.keras as keras\n",
    "# from tensorflow.contrib.keras.models import Sequential\n",
    "# from tensorflow.contrib.keras.layers import Dense, GRU, Embedding\n",
    "# from tensorflow.contrib.keras.optimizers import Adam\n",
    "# from tensorflow.contrib.keras.preprocessing.text import Tokenizer\n",
    "# from tensorflow.contrib.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 25000\n",
      "25000 25000\n",
      "Well, I just discovered that there is a show more disgusting and shocking than \"Little Britain\" and I like it! \"The League of Gentlemen\" is a sick British comedy that is about the most awful, insane and disgusting small town in all the UK. This place makes Dibley and Craggy Island (from \"The Vicar of Dibley\" and \"Father Ted\") seem pretty normal!! The format of the show is a lot like LITTLE Britain except that all of it centers around the townspeople of this one hellish town. Both shows feature the same skits again and again every episode and some obviously inspired \"Little Britain\" (particularly the job seeking class skit). But the show differs because although it is crude like \"Little Britain\" (hence not a show for kids), the show has a sick and sadistic quality that sets it apart from all these shows. In particular, animal cruelty and serial killing are recurring themes throughout the show.<br /><br />Now if you haven't guessed, this is NOT a show for kids, the easily offended or normal people and that's probably why I liked it. However, you really do need very thick skin and a love of the awful to enjoy this to the max. Funny and incredibly irreverent beyond belief--you have to see it to believe it. 1\n",
      "I cannot understand the need to jump backwards and forwards to scene set, and pad out the plot. Showing that someone has a skill right before they use it, I believe, is offending our intelligence. It's starting to feel a little contrived, and as though they are making up for being so vague for the first three series. A little disappointing this episode.<br /><br />Furthermore, using past quirks, like Locke's ability to know when a storm is ending, is frankly insulting... are we supposed to ooh and arr, or laugh at the softer side of Locke?<br /><br />This episode was all over the place. 1\n"
     ]
    }
   ],
   "source": [
    "def create_labels(data):\n",
    "    data_flat = []\n",
    "    labels = []\n",
    "    for k, v in data.items():\n",
    "        if k == \"pos\":\n",
    "            data_flat.extend(v)\n",
    "            labels.extend([1 for _ in range(len(v))])\n",
    "        if k == \"neg\":\n",
    "            data_flat.extend(v)\n",
    "            labels.extend([0 for _ in range(len(v))])\n",
    "    return data_flat, labels\n",
    "\n",
    "# creating test and train db\n",
    "\n",
    "X_train, y_train = create_labels(filenames[\"train\"])\n",
    "X_test, y_test = create_labels(filenames[\"test\"])\n",
    "\n",
    "print(len(X_train), len(y_train))\n",
    "print(len(X_test), len(y_test))\n",
    "\n",
    "print((X_train)[0], (y_train)[0])\n",
    "print((X_test)[0], (y_test)[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I cannot understand the need to jump backwards and forwards to scene set, and pad out the plot. Showing that someone has a skill right before they use it, I believe, is offending our intelligence. It's starting to feel a little contrived, and as though they are making up for being so vague for the first three series. A little disappointing this episode.<br /><br />Furthermore, using past quirks, like Locke's ability to know when a storm is ending, is frankly insulting... are we supposed to ooh and arr, or laugh at the softer side of Locke?<br /><br />This episode was all over the place. \n",
      "\n",
      "\n",
      "This movie is so bad that it actually gets funny. One of the worst movies I've ever seen in my entire life. The funny thing was that the trailer had scenes in it that wasn't in the movie. Just by watching the trailer I would have saved a lot of my time. It actually showed everything that happened in the movie except for the conclusion and that was also so obvious.<br /><br />It's honestly hard to think of a reason why this movie was made. This is just so bad. Horrible. <br /><br />I would give it 0 out of 10 if that would be possible. There is nothing else to say about this movie. \n",
      "\n",
      "\n",
      "Well, I just discovered that there is a show more disgusting and shocking than \"Little Britain\" and I like it! \"The League of Gentlemen\" is a sick British comedy that is about the most awful, insane and disgusting small town in all the UK. This place makes Dibley and Craggy Island (from \"The Vicar of Dibley\" and \"Father Ted\") seem pretty normal!! The format of the show is a lot like LITTLE Britain except that all of it centers around the townspeople of this one hellish town. Both shows feature the same skits again and again every episode and some obviously inspired \"Little Britain\" (particularly the job seeking class skit). But the show differs because although it is crude like \"Little Britain\" (hence not a show for kids), the show has a sick and sadistic quality that sets it apart from all these shows. In particular, animal cruelty and serial killing are recurring themes throughout the show.<br /><br />Now if you haven't guessed, this is NOT a show for kids, the easily offended or normal people and that's probably why I liked it. However, you really do need very thick skin and a love of the awful to enjoy this to the max. Funny and incredibly irreverent beyond belief--you have to see it to believe it. \n",
      "\n",
      "\n",
      "I can sum this movie up using 20 words or less. Way too predictable of a story line with potential to be funny but instead falls flat on its face. See, 19 words, however, I didn't completely pan this flick with just one star but instead decided to bump it up to two stars due to the fact that Julie Bowen is smoking hot and provided just enough eye candy to keep me from ripping the DVD right out from the machine and blowing it up with an M80. My advice, take the $4.00 rental fee you would have paid to see this movie and just send it right to me as an advance thank you for saving you the time and frustration of having to sit through this train wreck, or you may want to send me the $50.00 replacement fee you would have been charged from taking out your twelve gage to use this piece of garbage as skeet shooting practice. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(filenames[\"test\"][\"pos\"][0], \"\\n\\n\")\n",
    "print(filenames[\"test\"][\"neg\"][0], \"\\n\\n\")\n",
    "print(filenames[\"train\"][\"pos\"][0], \"\\n\\n\")\n",
    "print(filenames[\"train\"][\"neg\"][0], \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 12500.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,  12500.]),\n",
       " array([ 0. ,  0.1,  0.2,  0.3,  0.4,  0.5,  0.6,  0.7,  0.8,  0.9,  1. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8dc3790320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax2.hist(y_train)\n",
    "ax1.hist(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n"
     ]
    }
   ],
   "source": [
    "# Tokenize\n",
    "\n",
    "num_words = 10000\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(nb_words=num_words)\n",
    "\n",
    "data_text = X_train + X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.36 s, sys: 0 ns, total: 7.36 s\n",
      "Wall time: 7.36 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenizer.fit_on_texts(data_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oberoi 52583\n",
      "michal 48382\n",
      "cit's 67940\n",
      "belknap 53400\n",
      "stubs 96717\n",
      "allegra 17507\n",
      "erice's 113061\n",
      "swaying 22258\n",
      "'smart 42719\n",
      "124252\n"
     ]
    }
   ],
   "source": [
    "i = 10\n",
    "for k, v in tokenizer.word_index.items():\n",
    "    \n",
    "    i -= 1\n",
    "    if i > 0:\n",
    "        print(k, v)\n",
    "print (abs(i) + 10)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221.27716\n",
      "2208\n"
     ]
    }
   ],
   "source": [
    "x_train_tokens = tokenizer.texts_to_sequences(X_train)\n",
    "x_test_tokens = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "\n",
    "\n",
    "num_tokens = [len(tokens) for tokens in x_train_tokens + x_test_tokens]\n",
    "num_tokens = np.array(num_tokens)\n",
    "print(np.mean(num_tokens))\n",
    "print(np.max(num_tokens))\n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "max_tokens = int(max_tokens)\n",
    "max_tokens\n",
    "\n",
    "\n",
    "pad = 'pre'\n",
    "\n",
    "x_train_pad = keras.preprocessing.sequence.pad_sequences(x_train_tokens, maxlen=max_tokens,\n",
    "                            padding=pad, truncating=pad)\n",
    "\n",
    "\n",
    "x_test_pad = keras.preprocessing.sequence.pad_sequences(x_test_tokens, maxlen=max_tokens,\n",
    "                           padding=pad, truncating=pad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 544)\n",
      "(25000, 544)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_pad.shape)\n",
    "print(x_test_pad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 8 544\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "layer_embedding (Embedding)  (None, 544, 8)            80000     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 544, 16)           1200      \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 544, 8)            600       \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (None, 4)                 156       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 81,961\n",
      "Trainable params: 81,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "\n",
    "embedding_size = 8\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "\n",
    "print(num_words, embedding_size, max_tokens)\n",
    "model.add(keras.layers.Embedding(input_dim=num_words,\n",
    "                    output_dim=embedding_size,\n",
    "                    input_length=max_tokens,\n",
    "                    name='layer_embedding'))\n",
    "\n",
    "\n",
    "\n",
    "model.add(keras.layers.GRU(units=16, return_sequences=True))\n",
    "\n",
    "model.add(keras.layers.GRU(units=8, return_sequences=True))\n",
    "model.add(keras.layers.GRU(units=4))\n",
    "\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "\n",
    "optimizer = keras.optimizers.Adam(lr=1e-3)\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23750 samples, validate on 1250 samples\n",
      "Epoch 1/3\n",
      "23750/23750 [==============================] - 426s 18ms/step - loss: 0.6619 - acc: 0.5739 - val_loss: 0.5992 - val_acc: 0.6856\n",
      "Epoch 2/3\n",
      "23750/23750 [==============================] - 405s 17ms/step - loss: 0.4371 - acc: 0.8016 - val_loss: 0.3016 - val_acc: 0.8776\n",
      "Epoch 3/3\n",
      "23750/23750 [==============================] - 412s 17ms/step - loss: 0.3061 - acc: 0.8786 - val_loss: 0.3219 - val_acc: 0.8664\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x7f8dc3481be0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "epochs = 3\n",
    "\n",
    "model.fit(x_train_pad, y_train, validation_split=0.05, batch_size=batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 203s 8ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.33020338443756103, 0.86095999999999995]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test_pad, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1)\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_test_pad[:1000])\n",
    "print(y_pred.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[  0   6  17  30  31  42  47  55  68  82  86  88 100 104 122 123 126 133\n",
      " 157 159 161 165 170 178 196 197 199 201 203 209 211 218 225 229 239 252\n",
      " 255 256 259 267 276 290 310 313 321 334 353 354 360 368 370 382 387 401\n",
      " 410 419 425 426 427 433 434 438 444 445 451 455 462 485 501 510 512 519\n",
      " 521 526 531 535 538 539 540 551 552 554 561 573 577 581 585 591 599 605\n",
      " 619 628 633 640 647 649 652 657 664 678 681 682 685 689 692 695 702 703\n",
      " 708 711 714 723 739 741 752 760 763 767 772 774 777 785 802 811 828 829\n",
      " 833 845 846 847 853 858 867 876 877 879 893 898 899 917 919 926 927 930\n",
      " 931 932 944 952 970 974 980 985 995 999]\n"
     ]
    }
   ],
   "source": [
    "y_pred = y_pred.T[0]\n",
    "y_pred = np.array([1.0 if p>0.5 else 0.0 for p in y_pred])\n",
    "y_true = np.array(y_test[:1000])\n",
    "\n",
    "incorrect = np.where(y_pred != y_true)\n",
    "\n",
    "print(len(incorrect))\n",
    "\n",
    "print(incorrect[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(incorrect[0])\n",
    "incorrect[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,   10,  576,  385,    1,  362,    5, 1902,\n",
       "       6013,    2,    5,  129,  268,    2, 6876,   41,    1,  111,  761,\n",
       "         12,  296,   45,    3, 2788,  203,  159,   33,  356,    9,   10,\n",
       "        262,    6,  260, 1713,   44, 1843,    5,  229,    3,  120, 2129,\n",
       "          2,   14,  151,   33,   23,  232,   53,   15,  109,   34, 3589,\n",
       "         15,    1,   86,  277,  204,    3,  120, 1326,   11,  398,    7,\n",
       "          7, 3840,  783,  509, 9744,   37, 1239,    5,  118,   50,    3,\n",
       "       3050,    6,  272,    6, 1982, 3666,   23,   73,  442,    5,    2,\n",
       "         38,  423,   30,    1,  486,    4, 8851,    7,    7,   11,  398,\n",
       "         13,   29,  121,    1,  274], dtype=int32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_pad[incorrect[0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def token_to_text(tokens):\n",
    "rev_index = {v: k for k, v in tokenizer.word_index.items()}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beautiful this movie the bad it's genre of films br br the only two talents in it are richard boone and joan van ark and only boone is any good it's kind of sad that the man who rose to fame as should wind up in this ugly pile of celluloid while he turns in a fantastic performance i couldn't help but feel that he so all his fellow actors in this piece that he shouldn't even have been there br br the effects in this film are laughable but fun the idea of a dinosaur being buried in the wall of a cave and suddenly coming to life is b movie gold when the gets killed watch how it falls it's clear that the stunt performer in the front of the costume knows the timing best he falls to the ground well before the back half of the dinosaur follows suit br br speaking of there is nothing good to say about the purple in this flick it seems to have some kind of technology since to be the best in the world twice fails to notice it until it's within biting range of him i don't know how all the prints are but in the version i own the contains trademark br br this is loads of fun to watch if you like bad movies i love them and especially bad monster movies so i consider this the gem of my collection if bad movies are your thing definitely get this one\n",
      "0.0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "def token_to_text(tokens):\n",
    "    txt = []\n",
    "    for tok in tokens:\n",
    "#         print(tok)\n",
    "        txt.append(rev_index[tok] if tok in rev_index else None)\n",
    "    return \" \".join(list(filter(lambda x: x, txt)))\n",
    "# print(rev_index)\n",
    "cur = incorrect[0][1]\n",
    "print(token_to_text(x_test_pad[cur]))\n",
    "print(y_pred[cur])\n",
    "print(y_true[cur])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
